{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Climate Data Analysis with Azure Synapse and Azure ML\n",
        "\n",
        "In this notebook, I work with daily weather observations stored in Azure. The goal is to:\n",
        "\n",
        "- Connect to an Azure Synapse serverless SQL pool from an Azure Machine Learning notebook.\n",
        "- Explore climate data that has already been ingested and optimized into Parquet tables.\n",
        "- Focus on one weather station (USW00094728 – New York Central Park) and analyze its daily precipitation and daily maximum temperature time series.\n",
        "- Calculate yearly totals/averages and compare the differences between 2023, 2024, and 2025.\n",
        "- Repeat the same type of analysis for a second station of my choice.\n",
        "\n",
        "The heavy data ingestion and transformation work is done in Synapse (using external tables and CETAS). This notebook focuses on loading the prepared tables, visualizing the data, and interpreting the results.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## creenshots are in word File"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "## Summary and ETL Explanation\n",
        "\n",
        "In this notebook, I used Azure Synapse and Azure Machine Learning together to analyze weather data for specific stations.\n",
        "\n",
        "First, the raw CSV files (daily GHCN observations for multiple years) were uploaded to an Azure Data Lake storage account. In Synapse, these raw files were exposed as an external table (`ghcn_raw.dbo.raw`). Then, a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement was used to convert the raw CSV data into a Parquet table (`climate_parquet`) in the `ghcn_processed` database. From this optimized table, I created two smaller external tables: `prcp_series` (daily precipitation) and `tmax_series` (daily maximum temperature) for station USW00094728.\n",
        "\n",
        "In the second part, this notebook connected to the Synapse serverless SQL endpoint using a lightweight Azure ML compute. I loaded `prcp_series` and `tmax_series` into pandas DataFrames, plotted the daily time series, and then aggregated the data by year. I calculated yearly totals for precipitation and yearly averages for maximum temperature, and compared the differences between 2024–2023 and 2025–2024. I also repeated the same procedure for another station to compare patterns between two locations.\n",
        "\n",
        "From an ETL point of view:\n",
        "\n",
        "- **Extract:** reading raw CSV files from the `raw` container in the storage account into Synapse (`ghcn_raw.dbo.raw`).\n",
        "- **Transform:** converting CSV to Parquet with CETAS (`climate_parquet`) and filtering out only the needed columns and rows per station into `prcp_series` and `tmax_series`.\n",
        "- **Load:** writing these transformed datasets back into the `processed/` container as Parquet files and exposing them as external tables in `ghcn_processed`, which are then consumed by this notebook.\n",
        "\n",
        "This approach keeps the heavy data processing inside Synapse, while the Azure ML notebook only reads small, station-level tables. This makes the analysis cheaper, faster, and easier to run even on small compute.\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}